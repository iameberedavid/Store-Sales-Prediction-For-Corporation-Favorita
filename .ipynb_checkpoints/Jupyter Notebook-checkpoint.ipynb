{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b418d15f",
   "metadata": {},
   "source": [
    "# TIME SERIES FORECASTING ANALYSIS FOR CORPORATION FAVORITA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fab111",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "The primary focus of this project is to utilize time series regression analysis to forecast sales for Corporation Favorita, a prominent grocery retailer based in Ecuador.\n",
    "\n",
    "The objective is to develop a robust model capable of accurately forecasting future sales by leveraging the extensive time series data of thousands of products sold across various Corporation Favorita locations. The resulting forecasts will provide valuable insights to the store's management, enabling them to formulate effective inventory and sales plans.\n",
    "\n",
    "I will utilize the CRISP-DM framework to execute the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1774aa9",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "Sales is the primary parameter for measuring the success of any business, and Corporation Favorita is no exception. The business has the data of its sales from 2013 to 2017 across its stores located in different cities in Ecuador.\n",
    "\n",
    "During this time period, there have been holiday events, increases in oil price, and an earthquake in Ecuador. The grocery store management will like to know the impact of these occurrences on its sales. The management also wants to know the sales performance of its stores across different store types, clusters, cities and states.\n",
    "\n",
    "Furthermore, the management will like to know if there is a trend in the sales data, as well as the occurrence of seasonality to help identify the buying patterns of customers and prepare adequately for periods in the future where peak sales usually occur. The impact of promotion on sales is another key factor the business wants to derive insights on.\n",
    "\n",
    "This project is aimed at providing these business insights by performing regression analysis using the available time-stamped historical data. In addition, a scientific hypothesis will be established, and an optimal time-series regression model will be built to predict future sales. This prediction will ensure strategic decision-making in the future.\n",
    "\n",
    "By delving into the project, my goal is to support the management team of Corporation Favorita in extracting meaningful insights from their vast data, optimize operations and ultimately achieve sales growth, and help drive data-informed decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b03023",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "\n",
    "Null Hypothesis: Series is not stationary.\n",
    "\n",
    "Alternate Hypothesis: Series is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f35455",
   "metadata": {},
   "source": [
    "# Analytical Questions\n",
    "\n",
    "1. Is the train dataset complete (has all the required dates)?\n",
    "\n",
    "2. Which dates have the lowest and highest sales for each year?\n",
    "\n",
    "3. Did the earthquake impact sales?\n",
    "\n",
    "4. Are certain groups of stores selling more products? (Cluster, city, state, type)\n",
    "\n",
    "5. Are sales affected by promotions, oil prices and holidays?\n",
    "\n",
    "6. What analysis can we get from the date and its extractable features?\n",
    "\n",
    "7. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)\n",
    "\n",
    "8. What is the total sales made each year by the corporation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "230fc63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installations\n",
    "\n",
    "# !pip install pyodbc\n",
    "# !pip install python-dotenv\n",
    "# !pip install sqlalchemy\n",
    "# !pip install lightgbm\n",
    "# !pip install xgboost\n",
    "# !pip install catboost\n",
    "# !pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd3d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as ndates\n",
    "import seaborn as sns\n",
    "\n",
    "# Libraries to create connection string to SQL server\n",
    "import pyodbc\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Library for seasonal decomposition\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Library for checking stationarity\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Library for feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Library for feature encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Libraries for modelling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "import statsmodels.api as sm\n",
    "import pmdarima as pm\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Libraries for calculating evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "\n",
    "# Library to make series stationary\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Library for working with operating system\n",
    "import os\n",
    "\n",
    "# Library to handle warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "829015cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rc(\n",
    "    \"figure\",\n",
    "    autolayout=True,\n",
    "    figsize=(11, 4),\n",
    "    titlesize=18,\n",
    "    titleweight='bold',\n",
    ")\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    "    legend=False,\n",
    ")\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c76e14",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "### Accessing and loading the datasets\n",
    "\n",
    "The first dataset was collected from a SQL database by first passing a connection string using the pyodbc library. Afterwards a SQL query was used to obtain the dataset. This is as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b7ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variable in the .env file into a dictionary\n",
    "\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "# Get the values for the credentials you set in the .env file\n",
    "server = environment_variables.get('SERVER')\n",
    "database = environment_variables.get('DATABASE')\n",
    "username = environment_variables.get('USERNAME')\n",
    "password = environment_variables.get('PASSWORD')\n",
    "\n",
    "# The connection string is an f string that includes all the variable above to establish a connection to the server.\n",
    "connection_string = f'DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a79cebe",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "('08001', '[08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]SQL Server does not exist or access denied. (17) (SQLDriverConnect); [08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionOpen (Connect()). (53)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use the connect method of the pyodbc library to pass in the connection string.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Check your internet connection if it takes more time than necessary.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[43mpyodbc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Get the oil dataset using the SQL query shown below\u001b[39;00m\n\u001b[0;32m      7\u001b[0m query1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelect * from dbo.oil\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mOperationalError\u001b[0m: ('08001', '[08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]SQL Server does not exist or access denied. (17) (SQLDriverConnect); [08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionOpen (Connect()). (53)')"
     ]
    }
   ],
   "source": [
    "# Use the connect method of the pyodbc library to pass in the connection string.\n",
    "# Check your internet connection if it takes more time than necessary.\n",
    "\n",
    "connection = pyodbc.connect(connection_string)\n",
    "\n",
    "# Get the oil dataset using the SQL query shown below\n",
    "query1 = 'Select * from dbo.oil'\n",
    "oil = pd.read_sql(query1, connection)\n",
    "\n",
    "# Get the holiday dataset using the SQL query shown below\n",
    "query2 = 'Select * from dbo.holidays_events'\n",
    "holiday = pd.read_sql(query2, connection)\n",
    "\n",
    "# Get the stores dataset using the SQL query shown below\n",
    "query3 = 'Select * from dbo.stores'\n",
    "stores = pd.read_sql(query3, connection)\n",
    "\n",
    "# Save the datasets\n",
    "oil.to_csv(r'oil.csv')\n",
    "holiday.to_csv(r'holiday.csv')\n",
    "stores.to_csv(r'stores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdded6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "\n",
    "connection.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57598695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the other datasets\n",
    "\n",
    "transactions = pd.read_csv('transactions.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d681dc",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ba797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the holiday dataset\n",
    "\n",
    "holiday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the oil dataset\n",
    "\n",
    "oil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c00cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trend of oil prices\n",
    "\n",
    "oil_df = oil.set_index('date')\n",
    "oil_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the stores dataset\n",
    "\n",
    "stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb6543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the transactions dataset\n",
    "\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6186a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the train dataset\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af332e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the test dataset\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the sample_submission dataset\n",
    "\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d622b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of the datasets\n",
    "\n",
    "data = {'holiday': holiday, 'oil': oil, 'stores': stores, 'transactions': transactions, 'train': train, 'test': test, 'sample_submission': sample_submission}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d386d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check the datatypes and presence of missing values in each of the datasets\n",
    "# Use '\\033[1mtext\\033[0m' to make text bold\n",
    "\n",
    "for df, dataset in data.items():\n",
    "    print(f'\\033[1mFor {df} dataset\\033[0m:')\n",
    "    dataset.info()\n",
    "    print('_'*45)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape, and the presence of missing values and duplicates in each of the datasets\n",
    "# Use '\\033[1mtext\\033[0m' to make text bold\n",
    "\n",
    "for df, dataset in data.items():\n",
    "    print(f'\\033[1mFor {df} dataset\\033[0m')\n",
    "    print(f'Shape: {dataset.shape}')\n",
    "    print(f'Missing values = {dataset.isna().sum().sum()}')\n",
    "    print(f'Duplicates = {dataset.duplicated().sum()}')\n",
    "    print('_'*30)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3cf4ba",
   "metadata": {},
   "source": [
    "# Problems Identified\n",
    "\n",
    "The datasets are seperate, and need to be merged together for better analysis.\n",
    "\n",
    "The oil dataset has 43 missing values on the 'dcoilwtico' column which should be filled.\n",
    "\n",
    "There is no column to precisely state the days where there were holidays and days where there were no holidays. This column will be created and named 'holiday_status'.\n",
    "\n",
    "The 'date' columns have an object datatype instead of a datetime datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38a92d6",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "The problems identified with the datasets will be handled to prepare the data for analysis and modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b4579",
   "metadata": {},
   "source": [
    "### Merge the datasets based on common columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c11a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge transactions dataset to train on 'date' and 'store_nbr' columns\n",
    "df1 = pd.merge(train, transactions, on=['date', 'store_nbr'], how='left')\n",
    "\n",
    "# Merge holiday dataset to df1 on 'date' column\n",
    "df2 = pd.merge(df1, holiday, on='date', how='left')\n",
    "\n",
    "# Merge oil dataset to df2 on 'date' column\n",
    "df3 = pd.merge(df2, oil, on='date', how='left')\n",
    "df3\n",
    "\n",
    "# Merge store dataset to df3 on 'store_nbr' column\n",
    "df4 = pd.merge(df3, stores, on='store_nbr', how='left')\n",
    "\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates after merging the datasets\n",
    "\n",
    "df4.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0376f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataset\n",
    "\n",
    "df = df4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating 'type_x' column on df4\n",
    "\n",
    "df['type_x'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3141032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating 'type_y' column on df4\n",
    "\n",
    "df['type_y'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46e3d8",
   "metadata": {},
   "source": [
    "As can be seen in the merged dataset, the column named type_x is the type column of the holiday dataset, the column named dcoilwtico represents the oil price in the oil dataset, while the column named type_y is the type column of the store dataset. These columns will be renamed for easy identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'type_x', 'dcoilwtico' and type_y' to 'holiday_type', 'oil_price' and 'store_type' respectively\n",
    "\n",
    "df = df.rename(columns={'type_x': 'holiday_type', 'dcoilwtico': 'oil_price', 'type_y': 'store_type'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a133436",
   "metadata": {},
   "source": [
    "### Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab552692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values after merging the datasets\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f4201",
   "metadata": {},
   "source": [
    "The missing values in the transactions column will be filled with 0 because it represents the absence of transactions on those days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbe438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values in the transactions column with 0\n",
    "\n",
    "df['transactions'].fillna(0, inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4688fe9",
   "metadata": {},
   "source": [
    "For holiday_type, locale, locale_name, description and transferred columns, there are equal number of missing values. This is because these columns are from the holiday dataset, and they represent the days where there were no holidays. These empty cells will be filled with 'No holiday' for easy identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values in type_x, locale, locale_name, description and transferred columns with 'No holiday'\n",
    "columns_to_fill = ['holiday_type', 'locale', 'locale_name', 'description', 'transferred']\n",
    "for column in columns_to_fill:\n",
    "        df[column].fillna('No holiday', inplace=True)\n",
    "\n",
    "# Confirm that there are no more missing values in these columns\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6938fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trend of oil prices before filling the missing values\n",
    "\n",
    "df['oil_price'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b693750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values in the oil prices using backward fill to ensure continuity in the trend\n",
    "df['oil_price'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Confirm that there are no more missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05222fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trend of oil prices after filling the missing values\n",
    "\n",
    "df['oil_price'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25996a6d",
   "metadata": {},
   "source": [
    "### Create the 'holiday_status' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values of the 'holiday_type' column\n",
    "\n",
    "df['holiday_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map holiday type to holiday status\n",
    "def map_holiday_status(holiday_type):\n",
    "    if holiday_type in ['No holiday', 'Work Day']:\n",
    "        return 'No holiday'\n",
    "    else:\n",
    "        return 'Holiday'\n",
    "\n",
    "# Add the 'holiday_status' column using the 'holiday_type' column\n",
    "df['holiday_status'] = df['holiday_type'].apply(map_holiday_status)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values of the 'holiday_status' column\n",
    "\n",
    "df['holiday_status'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209e11af",
   "metadata": {},
   "source": [
    "### Change the datatype of the 'date' column from object to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the datatype of the date column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2dc1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features from the 'date' column using pandas' powerful time-based indexing\n",
    "\n",
    "df['year'] = df.date.dt.year\n",
    "df['month'] = df.date.dt.month\n",
    "df['dayofmonth'] = df.date.dt.day\n",
    "df['dayofweek'] = df.date.dt.dayofweek\n",
    "df['dayname'] = df.date.dt.strftime('%A')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc61dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'date' column as index\n",
    "\n",
    "df = df.set_index('date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a9fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename df to train_merged\n",
    "\n",
    "train_merged = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random sample of 8 rows\n",
    "\n",
    "train_merged.sample(8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 'transactions' column\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = train_merged['transactions'].plot(linewidth=0.5)\n",
    "ax.set_ylabel('Transactions')\n",
    "ax.set_xlabel('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065742dc",
   "metadata": {},
   "source": [
    "The plot above reveals that transactions are always highest at the end of each year. This reveals seasonality in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2566ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform seasonal decomposition\n",
    "\n",
    "result = seasonal_decompose(train_merged['sales'], model='additive', period=365)\n",
    "result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lag plot\n",
    "\n",
    "pd.plotting.lag_plot(train_merged['transactions'], lag=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot of the 'transactions' column grouped by 'locale'\n",
    "sns.boxplot(x='transactions', y='locale', data=train_merged)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c350c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of the 'transactions' column\n",
    "train_merged.transactions.hist()\n",
    "\n",
    "# Add labels to the x-axis, y-axis, and title\n",
    "plt.xlabel('Transactions', fontsize=16)\n",
    "plt.ylabel('Frequency', fontsize=16)\n",
    "plt.title('Histogram of Transactions', fontsize=20)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b9ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sales trend of the dataset\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(x='date', y='sales', data=train_merged)\n",
    "plt.title('Sales Trend Of The Dataset')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333af892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics for numerical columns in train_data DataFrame\n",
    "\n",
    "train_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1429f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of train_data with numerical columns only\n",
    "train_merged_num = train_merged.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Calculate the correlation matrix of the numerical columns\n",
    "corr_matrix = train_merged_num.corr()\n",
    "\n",
    "# Visualizing the correlation matrix with a heatmap\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "\n",
    "# Save the chart as an image file\n",
    "# plt.savefig('Correlation of the numerical columns of the train dataset.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2b16c",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "Null Hypothesis: Series is not stationary.\n",
    "\n",
    "Alternate Hypothesis: Series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a98015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform the ADF test\n",
    "# adf_result = adfuller(train_merged['sales'])\n",
    "\n",
    "# # Print the test results\n",
    "# print(f'ADF Statistic: {adf_result[0]}')\n",
    "# print(f'p-value: {adf_result[1]}')\n",
    "# print(f'Critical Values: {adf_result[4]}')\n",
    "\n",
    "# # Perform hypothesis testing\n",
    "# alpha = 0.05  # Significance level\n",
    "\n",
    "# if p_value <= alpha:\n",
    "#     print(\"Reject the null hypothesis. Series is stationary.\")\n",
    "# else:\n",
    "#     print(\"Fail to reject the null hypothesis. Series is not stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d7fdfa",
   "metadata": {},
   "source": [
    "# Answering Analytical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee276df5",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Is the train dataset complete (has all the required dates)?\n",
    "\n",
    "2. Which dates have the lowest and highest sales for each year?\n",
    "\n",
    "3. Did the earthquake impact sales?\n",
    "\n",
    "4. Are certain groups of stores selling more products? (Cluster, city, state, type)\n",
    "\n",
    "5. Are sales affected by promotions, oil prices and holidays?\n",
    "\n",
    "6. What analysis can we get from the date and its extractable features?\n",
    "\n",
    "7. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)\n",
    "\n",
    "8. What is the total sales made each year by the corporation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e26edb",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "##### Is the train dataset complete (has all the required dates)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c5588",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check the range of the date column\n",
    "dates_range = pd.date_range(start=train_merged.index.min(), end=train_merged.index.max())\n",
    "\n",
    "# Check for missing dates in the dataset\n",
    "missing_dates = set(dates_range.date) - set(train_merged.index.unique())\n",
    "\n",
    "# Create a new dataframe with the dates_missing data\n",
    "missing_dates_df = pd.DataFrame(missing_dates)\n",
    "missing_dates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f56e02",
   "metadata": {},
   "source": [
    "The dataset has some missing dates. This means that it is not complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842644a4",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "##### Which dates have the lowest and highest sales for each year?\n",
    "\n",
    "The dates where the sales is 0 are most likely the dates that the stores were not open. Such days will not be seen as dates with the lowest sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by year\n",
    "grouped_data = train_merged.groupby('Year')\n",
    "\n",
    "# Step 4: Find the dates with the highest and lowest sales for each year, excluding zero sales\n",
    "result_data = pd.DataFrame(columns=['Year', 'Highest_Sales_Date', 'Highest_Sales', 'Lowest_Sales_Date', 'Lowest_Sales'])\n",
    "\n",
    "for year, group in grouped_data:\n",
    "    nonzero_sales_group = group[group['Sales'] > 0]\n",
    "    max_sale_date = nonzero_sales_group['Sales'].idxmax()\n",
    "    min_sale_date = nonzero_sales_group['Sales'].idxmin()\n",
    "    max_sale_value = nonzero_sales_group['Sales'].max()\n",
    "    min_sale_value = nonzero_sales_group['Sales'].min()\n",
    "    \n",
    "    result_data = result_data.append({\n",
    "        'Year': year,\n",
    "        'Highest_Sales_Date': max_sale_date,\n",
    "        'Highest_Sales': max_sale_value,\n",
    "        'Lowest_Sales_Date': min_sale_date,\n",
    "        'Lowest_Sales': min_sale_value\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Step 5: Print the result DataFrame\n",
    "print(result_data)\n",
    "\n",
    "# Step 6: Visualize the data on a chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.index, df['Sales'], label='Sales')\n",
    "plt.scatter(result_data['Highest_Sales_Date'], result_data['Highest_Sales'], color='green', label='Highest Sales')\n",
    "plt.scatter(result_data['Lowest_Sales_Date'], result_data['Lowest_Sales'], color='red', label='Lowest Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.title('Sales with Highest and Lowest for Each Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8959b249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fdd0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77e0fafc",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Did the earthquake impact sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5476e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time period before and after the earthquake\n",
    "pre_earthquake_start_date = '2016-04-01'\n",
    "pre_earthquake_end_date = '2016-04-15'\n",
    "post_earthquake_start_date = '2016-04-17'\n",
    "post_earthquake_end_date = '2016-04-30'\n",
    "\n",
    "# Filter the sales data before and after the earthquake\n",
    "pre_earthquake_sales = train_merged[(train_merged.index >= pre_earthquake_start_date) & (train_merged.index <= pre_earthquake_end_date)]\n",
    "post_earthquake_sales = train_merged[(train_merged.index >= post_earthquake_start_date) & (train_merged.index <= post_earthquake_end_date)]\n",
    "\n",
    "# Calculate the total sales before and after the earthquake\n",
    "pre_earthquake_total_sales = pre_earthquake_sales['sales'].sum()\n",
    "post_earthquake_total_sales = post_earthquake_sales['sales'].sum()\n",
    "\n",
    "# Visualize the sales data before and after the sales data\n",
    "labels = ['Pre-Earthquake', 'Post-Earthquake']\n",
    "total_sales = [pre_earthquake_total_sales, post_earthquake_total_sales]\n",
    "plt.bar(labels, total_sales)\n",
    "plt.xlabel('Before And After Earthquake')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.title('Impact Of Earthquake On Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2a11f",
   "metadata": {},
   "source": [
    "The plot above shows that there was a slight increase in sales after the earthquake. This means that the earthquake did not affect the sales negatively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d381900",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Are certain groups of stores selling more products? (Cluster, city, state, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7220fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales in the different store clusters\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='cluster', y='sales', data=train_merged)\n",
    "plt.title('Sales In Different Store Clusters')\n",
    "plt.xlabel('Store Clusters')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21819bb8",
   "metadata": {},
   "source": [
    "The plot above shows that stores in cluster 5 are making more sales than the stores in other clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b860ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales in different cities\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='city', y='sales', data=train_merged)\n",
    "plt.title('Sales In Different Cities')\n",
    "plt.xlabel('Cities')\n",
    "plt.ylabel('Sales')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29bb3b9",
   "metadata": {},
   "source": [
    "The plot above shows that stores in Quito are making more sales than stores in other cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aababf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales in different states\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='state', y='sales', data=train_merged)\n",
    "plt.title('Sales In Different States')\n",
    "plt.xlabel('States')\n",
    "plt.ylabel('Sales')\n",
    "plt.xticks(rotation = 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7077f5f3",
   "metadata": {},
   "source": [
    "The plot above shows that stores in Pichincha are making more sales than stores in other states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales in different store types\n",
    "\n",
    "store_types = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='store_type', y='sales', data=train_merged, order=store_types)\n",
    "plt.title('Sales In Different Store Types')\n",
    "plt.xlabel('Store Type')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a80024",
   "metadata": {},
   "source": [
    "The plot above shows that stores in Store type A are making more sales than stores in other store types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebbf59",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Are sales affected by promotions, oil prices and holidays?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bbb180",
   "metadata": {},
   "source": [
    "##### Impact of promotions on sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify families with at least one item on promotion and families with no item on promotion\n",
    "train_merged['onpromotion'] = train_merged['onpromotion'].apply(lambda x: 'No Promotion' if x == 0 else 'Promotion')\n",
    "\n",
    "# Group by promotion and sum the sales\n",
    "x = train_merged.groupby(['onpromotion'], as_index=False).agg({'sales':'sum'})\n",
    "\n",
    "# Plot the sales of promotion and non_promotion families)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(x.onpromotion, x.sales)\n",
    "plt.title('Impact Of Promotion On Sales')\n",
    "plt.xticks((0,1))\n",
    "plt.xlabel('Promotion Status')\n",
    "plt.ylabel('Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624b7c3",
   "metadata": {},
   "source": [
    "The plot above shows that sales are affected by promotion. Product families with items on promotion are being sold more than product families with no item on promotion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867cf027",
   "metadata": {},
   "source": [
    "##### Impact of oil prices on sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bed2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales with different oil prices\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.scatterplot(x='oil_price', y='sales', data=train_merged)\n",
    "plt.title('Sales With Different Oil Prices')\n",
    "plt.xlabel('Oil Prices')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0b8c34",
   "metadata": {},
   "source": [
    "The plot above shows that sales are affected by the oil prices. As shown, there are more number of sales and more volume of sales at lower oil prices than at higher oil prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de92032",
   "metadata": {},
   "source": [
    "##### Impact of holidays on sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'holiday_status' and calculate the average sales for each category\n",
    "average_sales_by_holiday_status = train_merged.groupby('holiday_status')['sales'].mean().reset_index()\n",
    "\n",
    "# Plot the average sales for holidays and no holidays\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='holiday_status', y='sales', data=average_sales_by_holiday_status, palette='pastel')\n",
    "plt.xlabel('Holiday Status')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.title('Average Sales on Holidays and No Holidays')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59394b4",
   "metadata": {},
   "source": [
    "There are higher sales on holidays than no holidays. This is because more people are free to go out for shopping on holidays than no holidays when they have to be at work or school."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943bec01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare the sales on holidays with sales on non-holidays for the different store types\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='store_type', hue='holiday_status', y='sales', data=train_merged, ci=None, order=store_types)\n",
    "plt.title('Sales On Holidays Vs Non-holidays For Each Store Type')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend(title='Holiday Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4779c",
   "metadata": {},
   "source": [
    "Across all the store types, there are more sales on holidays than no holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5ac0a",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What analysis can we get from the date and its extractable features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68232d74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# From the date and its extractable features, the average sales on different days of the week can be analyzed\n",
    "\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='dayname', y='sales', data=train_merged, order=days)\n",
    "plt.title('Sales On Different Days Of The Week')\n",
    "plt.xlabel('Days Of The Week')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2442b5",
   "metadata": {},
   "source": [
    "From the date and its extractable features, we can observe the sales according to the days of the week. As shown above, there are more sales during the weekends, with peak sales on Sundays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ddbf1",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddddcb9a",
   "metadata": {},
   "source": [
    "MAE (Mean Absolute Error) measures the average absolute difference between predicted and actual values. It is less sensitive to outliers but generally larger than RMSE and MSE.\n",
    "\n",
    "MSE (Mean Squared Error) measures the average squared difference between predicted and actual values. It gives more weight to large errors, making it sensitive to outliers.\n",
    "\n",
    "RMSE (Root Mean Squared Error) is the square root of MSE and has the same sensitivity to outliers as MSE but is more interpretable.\n",
    "\n",
    "RMSLE (Root Mean Squared Logarithmic Error) is useful when relative errors matter more than absolute errors and is less sensitive to outliers in the target variable.\n",
    "\n",
    "MAE is generally greater than RMSE and MSE because it does not penalize larger errors as heavily as squared-error metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f05791",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "What is the total sales made each year by the corporation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef8e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and sum of sales for each year\n",
    "\n",
    "sales_per_year = train_merged.groupby(['year'], as_index=False).agg({'sales':'sum'})\n",
    "sales_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee58d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sales made each year\n",
    "\n",
    "plt.bar(sales_per_year.year, sales_per_year.sales)\n",
    "plt.title('Sales Per Year',fontsize=14)\n",
    "plt.ylabel('Sales',fontsize=14)\n",
    "plt.xlabel('Year',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef1a67",
   "metadata": {},
   "source": [
    "From the data provided, Corporation Favorita made it's lowest sales in 2013 and its highest sales in 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54de21d",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52463b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the train_merged dataset on which to perform feature engineering\n",
    "\n",
    "train_data = train_merged.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f0db9",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all the columns of the dataset\n",
    "\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b3556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random sample of 5 rows to see the contents of the columns\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some columns and display the dataset\n",
    "\n",
    "columns_to_drop = ['id', 'store_nbr', 'holiday_type', 'locale', 'locale_name', 'description', 'transferred', 'city', 'store_type', 'cluster', 'month', 'dayofmonth', 'dayofweek', 'dayname']\n",
    "\n",
    "train_data = train_data.drop(columns_to_drop, axis=1)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511eddc7",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "\n",
    "The dataset will be splitted to training and validation sets using the time-based split. This will be done based on the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the years in the dataset\n",
    "\n",
    "train_data['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the years for the training set and validation set\n",
    "train_years = [2013, 2014, 2015, 2016]\n",
    "val_year = [2017]\n",
    "\n",
    "# Obtain the training set and validation set\n",
    "train_set = train_data.loc[(train_data['year'].isin(train_years) & train_data['year'].isin(train_years))]\n",
    "val_set = train_data.loc[(train_data['year'].isin(val_year) & train_data['year'].isin(val_year))]\n",
    "\n",
    "# Print the shape of the training set and validation set\n",
    "train_set.shape, val_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the datatypes of the columns of the training set\n",
    "\n",
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the original index column for later use\n",
    "train_index = train_set.index\n",
    "val_index = val_set.index\n",
    "\n",
    "# Obtain the categorical columns to encode and numerical columns to scale\n",
    "# Note that the 'sales' colum which is the target variable will not be scaled\n",
    "cat_columns_to_encode = ['family', 'onpromotion', 'state', 'holiday_status']\n",
    "num_columns_to_scale = ['transactions', 'oil_price']\n",
    "\n",
    "# Create seperate DataFrames for categorical columns and numerical columns of training set\n",
    "train_set_cat_df = train_set[cat_columns_to_encode]\n",
    "train_set_num_df = train_set[num_columns_to_scale]\n",
    "\n",
    "# Create seperate DataFrames for categorical columns and numerical columns of validation set\n",
    "val_set_cat_df = val_set[cat_columns_to_encode]\n",
    "val_set_num_df = val_set[num_columns_to_scale]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6877ec",
   "metadata": {},
   "source": [
    "### Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549da07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an encoder object using OneHotEncoder.\n",
    "# Set sparse=False for dense output and drop='first' to avoid multicollinearity\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to encode the categorical columns of the training set\n",
    "encoder.fit(train_set_cat_df)\n",
    "train_set_cat_encoded = encoder.transform(train_set_cat_df).tolist()\n",
    "train_set_cat_encoded_df = pd.DataFrame(train_set_cat_encoded, columns=encoder.get_feature_names_out(), index=train_index)\n",
    "\n",
    "# View the encoded columns of the training set\n",
    "train_set_cat_encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aed355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to encode the categorical columns of the validation set\n",
    "encoder.fit(val_set_cat_df)\n",
    "val_set_cat_encoded = encoder.transform(val_set_cat_df).tolist()\n",
    "val_set_cat_encoded_df = pd.DataFrame(val_set_cat_encoded, columns=encoder.get_feature_names_out(), index=val_index)\n",
    "\n",
    "# View the encoded columns of the validation set\n",
    "val_set_cat_encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb672032",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89815d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaler object using StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f480e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler to scale the numerical columns of the training set\n",
    "scaler.fit(train_set_num_df)\n",
    "train_set_num_scaled = scaler.transform(train_set_num_df).tolist()\n",
    "train_set_num_scaled_df = pd.DataFrame(train_set_num_scaled, columns=scaler.get_feature_names_out(), index=train_index)\n",
    "\n",
    "# View the scaled columns of the training set\n",
    "train_set_num_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed04e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler to scale the numerical columns of the validation set\n",
    "scaler.fit(val_set_num_df)\n",
    "val_set_num_scaled = scaler.transform(val_set_num_df).tolist()\n",
    "val_set_num_scaled_df = pd.DataFrame(val_set_num_scaled, columns=scaler.get_feature_names_out(), index=val_index)\n",
    "\n",
    "# View the scaled columns of the validation set\n",
    "val_set_num_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the date column of train_set and val_set with the encoded and scaled DataFrames to get the final training\n",
    "# and validation sets\n",
    "train_final = pd.concat([train_set_cat_encoded_df, train_set_num_scaled_df, train_set['sales']], axis=1)\n",
    "val_final = pd.concat([val_set_cat_encoded_df, val_set_num_scaled_df, val_set['sales']], axis=1)\n",
    "\n",
    "# Print the shape of the final training and validation sets\n",
    "train_final.shape, val_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062497ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first five rows of the final training set\n",
    "\n",
    "train_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d90355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that there are no missing values in the final training set\n",
    "\n",
    "train_final.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fadfde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first five rows of the final validation set\n",
    "\n",
    "val_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that there are no missing values in the final validation set\n",
    "\n",
    "val_final.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d9a021",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_values = train_data[train_data['sales'] < 0]\n",
    "\n",
    "# Display the rows with negative values in the target variable\n",
    "negative_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209609e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_negative_values = (train_data['sales'] < 0).sum()\n",
    "print(\"Number of negative values in the target variable:\", num_negative_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255da1a",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "The following models will be trained and evaluated:\n",
    "1. Linear Regression\n",
    "\n",
    "2. XGBoost\n",
    "\n",
    "3. CatBoost\n",
    "\n",
    "4. AutoRegressive (AR)\n",
    "\n",
    "5. AutoRegressive Integrated Moving Average (ARIMA)\n",
    "\n",
    "6. Seasonal AutoRegressive Integrated Moving Average (SARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the results of the evaluation metrics of each model\n",
    "\n",
    "results = pd.DataFrame(columns=['Model', 'MSE', 'RMSE', 'MSLE', 'RMSLE'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38053413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input features and target variable of the training set for Linear Regression, XGBoost, and CatBoost\n",
    "X_train = train_final.drop(columns=['sales'])\n",
    "y_train = train_final['sales']\n",
    "\n",
    "# Define the input features and target variable of the validation set for Linear Regression, XGBoost, and CatBoost\n",
    "X_val = val_final.drop(columns=['sales'])\n",
    "y_val = val_final['sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c397bcf9",
   "metadata": {},
   "source": [
    "### Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da54b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "linear_regression_model = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "linear_regression_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the final validation set\n",
    "linear_regression_pred = linear_regression_model.predict(X_val)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "linear_regression_mse = mean_squared_error(y_val, linear_regression_pred)\n",
    "linear_regression_rmse = np.sqrt(linear_regression_mse)\n",
    "linear_regression_msle = mean_squared_log_error(y_val, linear_regression_pred)\n",
    "linear_regression_rmsle = np.sqrt(linear_regression_msle)\n",
    "\n",
    "# Create a new row for the evaluation metrics of Linear Regression model on the results DataFrame\n",
    "linear_regression_results = pd.DataFrame([['Linear Regression', 'linear_regression_mse', 'linear_regression_rmse',\n",
    "                                           'linear_regression_msle', 'linear_regression_rmsle']],\n",
    "                                         columns=['Model', 'MSE', 'RMSE', 'MSLE', 'RMSLE'])\n",
    "results = results.append(linear_regression_results, ignore_index=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Linear Regression model performance\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(y_train, label='Train')\n",
    "plt.plot(y_val, label='Validation')\n",
    "plt.plot(linear_regression_pred, label='Linear Regression Forecast')\n",
    "plt.title('Linear Regression Model Performance')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be5791",
   "metadata": {},
   "source": [
    "### Model 2: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8903218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "xgboost_model = XGBRegressor()\n",
    "\n",
    "# Fit the model\n",
    "xgboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the final validation set\n",
    "xgboost_pred = xgboost_model.predict(X_val)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "xgboost_mse = mean_squared_error(y_val, xgboost_pred)\n",
    "xgboost_rmse = np.sqrt(xgboost_mse)\n",
    "xgboost_msle = mean_squared_log_error(y_val, xgboost_pred)\n",
    "xgboost_rmsle = np.sqrt(xgboost_msle)\n",
    "\n",
    "# Create a new row for the evaluation metrics of XGBoost model on the results DataFrame\n",
    "xgboost_results = pd.DataFrame([['XGBoost', 'xgboost_mse', 'xgboost_rmse', 'xgboost_msle', 'xgboost_rmsle']],\n",
    "                               columns=['Model', 'MSE', 'RMSE', 'MSLE', 'RMSLE'])\n",
    "results = results.append(xgboost_results, ignore_index=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863965a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize XGBoost model performance\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(y_train, label='Train')\n",
    "plt.plot(y_val, label='Validation')\n",
    "plt.plot(xgboost_pred, label='XGBoost Forecast')\n",
    "plt.title('XGBoost Model Performance')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f19f3f6",
   "metadata": {},
   "source": [
    "### Model 3: CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720fc8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "# Fit the model\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "catboost_pred = catboost_model.predict(X_val)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "catboost_mse = mean_squared_error(y_val, catboost_pred)\n",
    "catboost_rmse = np.sqrt(catboost_mse)\n",
    "catboost_msle = mean_squared_log_error(y_val, catboost_pred)\n",
    "catboost_rmsle = np.sqrt(catboost_msle)\n",
    "\n",
    "# Create a new row for the evaluation metrics of CatBoost model on the results DataFrame\n",
    "catboost_results = pd.DataFrame([['CatBoost', 'catboost_mse', 'catboost_rmse', 'catboost_msle', 'catboost_rmsle']],\n",
    "                                columns=['Model', 'MSE', 'RMSE', 'MSLE', 'RMSLE'])\n",
    "results = results.append(catboost_results, ignore_index=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b04e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CatBoost model performance\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(y_train, label='Train')\n",
    "plt.plot(y_val, label='Validation')\n",
    "plt.plot(catboost_pred, label='CatBoost Forecast')\n",
    "plt.title('CatBoost Model Performance')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d33545",
   "metadata": {},
   "source": [
    "### Model 4: AutoRegressive (AR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pacf and acf\n",
    "\n",
    "pacf = plot_pacf(train_data['sales'], lags=10)\n",
    "acf = plot_acf(train_data['sales'], lags=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "AR_model = AutoReg(train_final, lags=5)\n",
    "\n",
    "# Fit the model\n",
    "AR_model = AR_model.fit()\n",
    "AR_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb18a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "AR_preds = AR_model.predict(start=len(train_final), end=len(train_final)+len(val_final)-1, dynamic=False)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "AR_mse = mean_squared_error(val_final, AR_preds)\n",
    "AR_rmse = np.sqrt(AR_mse)\n",
    "AR_msle = mean_squared_log_error(val_final, AR_preds)\n",
    "AR_rmsle = np.sqrt(AR_msle)\n",
    "\n",
    "# Create a new row for the evaluation metrics of AR model on the results DataFrame\n",
    "AR_results = pd.DataFrame([['AR', 'AR_mse', 'AR_rmse', 'AR_msle', 'AR_rmsle']],\n",
    "                                columns=['Model', 'MSE', 'RMSE', 'MSLE', 'RMSLE'])\n",
    "results = results.append(AR_results, ignore_index=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c43257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize AR model performance\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(train_final['sales'], label='Train')\n",
    "plt.plot(val_final['sales'], label='Validation')\n",
    "plt.plot(AR_preds, label='AR Forecast')\n",
    "plt.title('AutoRegressive (AR) Model Performance')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07e55b",
   "metadata": {},
   "source": [
    "### Model 5: AutoRegressive Integrated Moving Average (ARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d025217",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_fit = autoarima(train_final['sales'], trace=True, suppress_warnings=True)\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50fda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "ARIMA_model = ARIMA(train_final, order=(1,1,1))\n",
    "\n",
    "# Fit the model\n",
    "ARIMA_model = ARIMA_model.fit()\n",
    "ARIMA_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809315a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "ARIMA_preds = ARIMA_model.predict(start=len(train_final), end=len(train_final)+len(val_final)-1, typ='levels')\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "ARIMA_mse = mean_squared_error(val_final, ARIMA_preds)\n",
    "ARIMA_rmse = np.sqrt(ARIMA_mse)\n",
    "ARIMA_msle = mean_squared_log_error(val_final, ARIMA_preds)\n",
    "ARIMA_rmsle = np.sqrt(ARIMA_msle)\n",
    "\n",
    "# Create a new row for the evaluation metrics of ARIMA model on the results DataFrame\n",
    "ARIMA_results = pd.DataFrame([['ARIMA', 'ARIMA_mse', 'ARIMA_rmse', 'ARIMA_msle', 'ARIMA_rmsle']],\n",
    "                                columns=['Model', 'MSE', 'RMSE', 'MSLE', 'RMSLE'])\n",
    "results = results.append(ARIMA_results, ignore_index=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ARIMA model performance\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(train_final['sales'], label='Train')\n",
    "plt.plot(val_final['sales'], label='Validation')\n",
    "plt.plot(ARIMA_preds, label='ARIMA Forecast')\n",
    "plt.title('AutoRegressive Integrated Moving Average (ARIMA) Model Performance')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48fcc57",
   "metadata": {},
   "source": [
    "### Model 6: Seasonal AutoRegressive Integrated Moving Average (SARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40272c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "SARIMA_model = SARIMAX(train, order=(1,1,1), seasonal_order=(1,1,1,12))\n",
    "\n",
    "# Fit the model\n",
    "SARIMA_model = SARIMA_model.fit()\n",
    "SARIMA_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f8197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "SARIMA_preds = SARIMA_model.predict(start=len(train_final), end=len(train_final)+len(val_final)-1, typ='levels')\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "SARIMA_mse = mean_squared_error(val_final, SARIMA_preds)\n",
    "SARIMA_rmse = np.sqrt(SARIMA_mse)\n",
    "SARIMA_msle = mean_squared_log_error(val_final, SARIMA_preds)\n",
    "SARIMA_rmsle = np.sqrt(SARIMA_msle)\n",
    "\n",
    "# Create a new row for the evaluation metrics of SARIMA model on the results DataFrame\n",
    "SARIMA_results = pd.DataFrame([['SARIMA', 'SARIMA_mse', 'SARIMA_rmse', 'SARIMA_msle', 'SARIMA_rmsle']],\n",
    "                                columns=['Model', 'MSE', 'RMSE', 'MSLE', 'RMSLE'])\n",
    "results = results.append(SARIMA_results, ignore_index=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SARIMA model performance\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(train_final['sales'], label='Train')\n",
    "plt.plot(val_final['sales'], label='Validation')\n",
    "plt.plot(SARIMA_preds, label='SARIMA Forecast')\n",
    "plt.title('Seasonal AutoRegressive Integrated Moving Average (SARIMA) Model Performance')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5321872",
   "metadata": {},
   "source": [
    "# Make series stationary\n",
    "\n",
    "We will use the boxcox method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32826081",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_boxcox = pd.Series(boxcox(train_data['sales'], lbda=0), index=train_data.index)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(data_boxcox, label='Before Boxcox Transformation')\n",
    "plt.title('Before Boxcox Transformation')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c35e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_boxcox_diff = pd.Series(data_boxcox - data_boxcox.shift(), index=train_data.index)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(data_boxcox_diff, label='After Boxcox Transformation')\n",
    "plt.title('After Boxcox Transformation')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad0bde",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cdc858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

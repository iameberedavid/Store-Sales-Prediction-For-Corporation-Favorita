{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b418d15f",
   "metadata": {},
   "source": [
    "# TIMESERIES ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fab111",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "This project focuses on the application of time series regression analysis to forecast sales for Corporation Favorita, a prominent grocery retailer based in Ecuador.\n",
    "\n",
    "The primary objective is to develop a robust model capable of accurately forecasting future sales by leveraging the extensive time series data of thousands of products sold across various Favorita locations. The resulting forecasts will provide valuable insights to the store's management, enabling them to formulate effective inventory and sales plans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1774aa9",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "Sales is the primary parameter for measuring the success of any business, and Corporation Favorita is no exception. The business has the data of its sales from 2013 to 2017 across its stores located in different cities in Ecuador. During this time period, there have been holiday events, increase in oil price and an earthquake in Ecuador. The business will like to know the impact of these occurrences on its sales, as well as the sales performance of its stores across different store types, clusters, cities and states.\n",
    "\n",
    "Furthermore, the business will like to know if there is a trend in the sales data, as well as the occurrence of seasonality to help identify the buying patterns of customers and prepare adequately for expected peak sales periods. The impact of promotions on sales is another key factor the business wants to derive insigts on.\n",
    "\n",
    "This project is aimed at providing these business insights by performing historical analysis using the available time-stamped historical data, establishing scientific hypothesis, and building an optimal time-series regression model to predict future sales. This prediction will ensure strategic decision-making in the future. By delving into the data, my goal is to optimize operations and ultimately drive sales growth for Corporation Favorita, support the management team in extracting meaningful insights from their vast data, and help drive data-informed decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b03023",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "\n",
    "Null Hypothesis: Sales are not affected by promotion, oil prices and holidays.\n",
    "\n",
    "Alternate Hypothesis: Sales are affected by promotion, oil prices and holidays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f35455",
   "metadata": {},
   "source": [
    "# Analytical Questions\n",
    "\n",
    "1. Is the train dataset complete (has all the required dates)?\n",
    "\n",
    "2. Which dates have the lowest and highest sales for each year?\n",
    "\n",
    "3. Did the earthquake impact sales?\n",
    "\n",
    "4. Are certain groups of stores selling more products? (Cluster, city, state, type)\n",
    "\n",
    "5. Are sales affected by promotions, oil prices and holidays?\n",
    "\n",
    "6. What analysis can we get from the date and its extractable features?\n",
    "\n",
    "7. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)\n",
    "\n",
    "8. What is the total sales made each year by the corporation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "230fc63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installations\n",
    "\n",
    "# !pip install pyodbc\n",
    "# !pip install python-dotenv\n",
    "# !pip install sqlalchemy\n",
    "# !pip install lightgbm\n",
    "# !pip install xgboost\n",
    "# !pip install catboost\n",
    "# !pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cd3d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as ndates\n",
    "\n",
    "# Libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Libraries to create connection string to SQL server\n",
    "import pyodbc\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Library for imputing missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Library for seasonal decomposition\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Library for checking stationarity\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Library for feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Library for feature encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Libraries for modelling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Libraries for calculating evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "\n",
    "# Library to make series stationary\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Library for working with operating system\n",
    "import os\n",
    "\n",
    "# Library to handle warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "829015cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rc(\n",
    "    \"figure\",\n",
    "    autolayout=True,\n",
    "    figsize=(11, 4),\n",
    "    titlesize=18,\n",
    "    titleweight='bold',\n",
    ")\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    "    legend=False,\n",
    ")\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c76e14",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "### Accessing and loading the datasets\n",
    "\n",
    "The first dataset was collected from a SQL database by first passing a connection string using the pyodbc library. Afterwards a SQL query was used to obtain the dataset. This is as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93b7ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variable in the .env file into a dictionary\n",
    "\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "# Get the values for the credentials you set in the .env file\n",
    "server = environment_variables.get('SERVER')\n",
    "database = environment_variables.get('DATABASE')\n",
    "username = environment_variables.get('USERNAME')\n",
    "password = environment_variables.get('PASSWORD')\n",
    "\n",
    "# The connection string is an f string that includes all the variable above to establish a connection to the server.\n",
    "connection_string = f'DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a79cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the connect method of the pyodbc library to pass in the connection string.\n",
    "# Check your internet connection if it takes more time than necessary.\n",
    "\n",
    "connection = pyodbc.connect(connection_string)\n",
    "\n",
    "# Get the oil dataset using the SQL query shown below\n",
    "query1 = 'Select * from dbo.oil'\n",
    "oil = pd.read_sql(query1, connection)\n",
    "\n",
    "# Get the holiday dataset using the SQL query shown below\n",
    "query2 = 'Select * from dbo.holidays_events'\n",
    "holiday = pd.read_sql(query2, connection)\n",
    "\n",
    "# Get the stores dataset using the SQL query shown below\n",
    "query3 = 'Select * from dbo.stores'\n",
    "stores = pd.read_sql(query3, connection)\n",
    "\n",
    "# Save the datasets\n",
    "oil.to_csv(r'oil.csv')\n",
    "holiday.to_csv(r'holiday.csv')\n",
    "stores.to_csv(r'stores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdded6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "\n",
    "connection.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57598695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the other datasets\n",
    "\n",
    "transactions = pd.read_csv('transactions.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d681dc",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ba797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the holiday dataset\n",
    "\n",
    "holiday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the oil dataset\n",
    "\n",
    "oil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c00cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trend of oil prices\n",
    "\n",
    "oil_df = oil.set_index('date')\n",
    "oil_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the stores dataset\n",
    "\n",
    "stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb6543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the transactions dataset\n",
    "\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6186a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the train dataset\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af332e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the test dataset\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the sample_submission dataset\n",
    "\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d622b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of the datasets\n",
    "\n",
    "data = {'holiday': holiday, 'oil': oil, 'stores': stores, 'transactions': transactions, 'train': train, 'test': test, 'sample_submission': sample_submission}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d386d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check the datatypes and presence of missing values in each of the datasets\n",
    "# Use '\\033[1mtext\\033[0m' to make text bold\n",
    "\n",
    "for df, dataset in data.items():\n",
    "    print(f'\\033[1mFor {df} dataset\\033[0m:')\n",
    "    dataset.info()\n",
    "    print('_'*45)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape, and the presence of missing values and duplicates in each of the datasets\n",
    "# Use '\\033[1mtext\\033[0m' to make text bold\n",
    "\n",
    "for df, dataset in data.items():\n",
    "    print(f'\\033[1mFor {df} dataset\\033[0m')\n",
    "    print(f'Shape: {dataset.shape}')\n",
    "    print(f'Missing values = {dataset.isna().sum().sum()}')\n",
    "    print(f'Duplicates = {dataset.duplicated().sum()}')\n",
    "    print('_'*30)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3cf4ba",
   "metadata": {},
   "source": [
    "# Problems Identified\n",
    "\n",
    "The datasets are seperate, and need to be merged together for better analysis.\n",
    "\n",
    "The oil dataset has 43 missing values on the 'dcoilwtico' column which should be filled.\n",
    "\n",
    "Each of the 'date' columns have an object datatype instead of a datetime datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38a92d6",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "The problems identified with the datasets will be handled to prepare the data for analysis and modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b4579",
   "metadata": {},
   "source": [
    "### Merge the datasets based on common columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c11a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge transactions dataset to train on 'date' and 'store_nbr' columns\n",
    "df1 = pd.merge(train, transactions, on=['date', 'store_nbr'], how='left')\n",
    "\n",
    "# Merge holiday dataset to df1 on 'date' column\n",
    "df2 = pd.merge(df1, holiday, on='date', how='left')\n",
    "\n",
    "# Merge oil dataset to df2 on 'date' column\n",
    "df3 = pd.merge(df2, oil, on='date', how='left')\n",
    "df3\n",
    "\n",
    "# Merge store dataset to df3 on 'store_nbr' column\n",
    "df4 = pd.merge(df3, stores, on='store_nbr', how='left')\n",
    "\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates after merging the datasets\n",
    "\n",
    "df4.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0376f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataset\n",
    "\n",
    "df = df4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating 'type_x' column on df4\n",
    "\n",
    "df['type_x'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3141032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating 'type_y' column on df4\n",
    "\n",
    "df['type_y'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46e3d8",
   "metadata": {},
   "source": [
    "As can be seen in the merged dataset, the column named type_x is the type column of the holiday dataset, the column named dcoilwtico represents the oil price in the oil dataset, while the column named type_y is the type column of the store dataset. These columns will be renamed for easy identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'type_x', 'dcoilwtico' and type_y' to 'holiday_type', 'oil_price' and 'store_type' respectively\n",
    "\n",
    "df = df.rename(columns={'type_x': 'holiday_type', 'dcoilwtico': 'oil_price', 'type_y': 'store_type'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a133436",
   "metadata": {},
   "source": [
    "### Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab552692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values after merging the datasets\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f4201",
   "metadata": {},
   "source": [
    "The missing values in the transactions column will be filled with 0 because it represents the absence of transactions on those days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbe438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values in the transactions column with 0\n",
    "\n",
    "df['transactions'].fillna(0, inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4688fe9",
   "metadata": {},
   "source": [
    "For holiday_type, locale, locale_name, description and transferred columns, there are equal number of missing values. This is because these columns are from the holiday dataset, and they represent the days where there were no holidays. These empty cells will be filled with 'No holiday' for easy identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values in type_x, locale, locale_name, description and transferred columns with 'No holiday'\n",
    "columns_to_fill = ['holiday_type', 'locale', 'locale_name', 'description', 'transferred']\n",
    "for column in columns_to_fill:\n",
    "        df[column].fillna('No holiday', inplace=True)\n",
    "\n",
    "# Confirm that there are no more missing values in these columns\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6938fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trend of oil prices before filling the missing values\n",
    "\n",
    "df['oil_price'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b693750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values in the oil prices using backward fill to ensure continuity in the trend\n",
    "df['oil_price'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Confirm that there are no more missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05222fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trend of oil prices after filling the missing values\n",
    "\n",
    "df['oil_price'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209e11af",
   "metadata": {},
   "source": [
    "### Change the datatype of the 'date' column from object to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the datatype of the date column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2dc1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features from the 'date' column using pandas' powerful time-based indexing\n",
    "\n",
    "df['year'] = df.date.dt.year\n",
    "df['month'] = df.date.dt.month\n",
    "df['dayofmonth'] = df.date.dt.day\n",
    "df['dayofweek'] = df.date.dt.dayofweek\n",
    "df['dayname'] = df.date.dt.strftime('%A')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc61dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'date' column as index\n",
    "\n",
    "df = df.set_index('date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a9fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename df to train_merged\n",
    "\n",
    "train_merged = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random sample of 8 rows\n",
    "\n",
    "train_merged.sample(8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 'transactions' column\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = train_merged['transactions'].plot(linewidth=0.5)\n",
    "ax.set_ylabel('Transactions')\n",
    "ax.set_xlabel('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065742dc",
   "metadata": {},
   "source": [
    "The plot above reveals that transactions are always highest at the end of each year. This reveals seasonality in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2566ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform seasonal decomposition\n",
    "\n",
    "result = seasonal_decompose(train_merged['sales'], model='additive', period=365)\n",
    "result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd21271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for stationarity using adfuller\n",
    "\n",
    "# result = adfuller(series, autolag='AIC')\n",
    "\n",
    "# print(f'ADF Statistics: {result[0]}')\n",
    "# print(f'p-value: {result[1]}')\n",
    "# print(f'Critical Values: {result[4]}')\n",
    "\n",
    "# if result[1] > 0.05 :\n",
    "#     print('Series is not stationary')\n",
    "# else:\n",
    "#     print('Series is stationary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lag plot\n",
    "\n",
    "pd.plotting.lag_plot(train_merged['transactions'], lag=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot of the 'transactions' column grouped by 'locale'\n",
    "sns.boxplot(x='transactions', y='locale', data=train_merged)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c350c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of the 'transactions' column\n",
    "train_merged.transactions.hist()\n",
    "\n",
    "# Add labels to the x-axis, y-axis, and title\n",
    "plt.xlabel('Transactions', fontsize=16)\n",
    "plt.ylabel('Frequency', fontsize=16)\n",
    "plt.title('Histogram of Transactions', fontsize=20)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b9ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sales trend of the dataset\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(x='date', y='sales', data=train_merged)\n",
    "plt.title('Sales Trend Of The Dataset')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333af892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics for numerical columns in train_data DataFrame\n",
    "\n",
    "train_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1429f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of train_data with numerical columns only\n",
    "train_merged_num = train_merged.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Calculate the correlation matrix of the numerical columns\n",
    "corr_matrix = train_merged_num.corr()\n",
    "\n",
    "# Visualizing the correlation matrix with a heatmap\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "\n",
    "# Save the chart as an image file\n",
    "# plt.savefig('Correlation of the numerical columns of the train dataset.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2b16c",
   "metadata": {},
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d7fdfa",
   "metadata": {},
   "source": [
    "# Answering Analytical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee276df5",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Is the train dataset complete (has all the required dates)?\n",
    "\n",
    "2. Which dates have the lowest and highest sales for each year?\n",
    "\n",
    "3. Did the earthquake impact sales?\n",
    "\n",
    "4. Are certain groups of stores selling more products? (Cluster, city, state, type)\n",
    "\n",
    "5. Are sales affected by promotions, oil prices and holidays?\n",
    "\n",
    "6. What analysis can we get from the date and its extractable features?\n",
    "\n",
    "7. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)\n",
    "\n",
    "8. What is the total sales made each year by the corporation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e26edb",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Is the train dataset complete (has all the required dates)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c5588",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check the range of the date column\n",
    "dates_range = pd.date_range(start=train_merged.index.min(), end=train_merged.index.max())\n",
    "\n",
    "# Check for missing dates in the dataset\n",
    "missing_dates = set(dates_range.date) - set(train_merged.index.unique())\n",
    "\n",
    "# Create a new dataframe with the dates_missing data\n",
    "missing_dates_df = pd.DataFrame(missing_dates)\n",
    "missing_dates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f56e02",
   "metadata": {},
   "source": [
    "The dataset has some missing dates. This means that it is not complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842644a4",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Which dates have the lowest and highest sales for each year?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0fafc",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Did the earthquake impact sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5476e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time period before and after the earthquake\n",
    "pre_earthquake_start_date = '2016-04-01'\n",
    "pre_earthquake_end_date = '2016-04-15'\n",
    "post_earthquake_start_date = '2016-04-17'\n",
    "post_earthquake_end_date = '2016-04-30'\n",
    "\n",
    "# Filter the sales data before and after the earthquake\n",
    "pre_earthquake_sales = train_merged[(train_merged.index >= pre_earthquake_start_date) & (train_merged.index <= pre_earthquake_end_date)]\n",
    "post_earthquake_sales = train_merged[(train_merged.index >= post_earthquake_start_date) & (train_merged.index <= post_earthquake_end_date)]\n",
    "\n",
    "# Calculate the total sales before and after the earthquake\n",
    "pre_earthquake_total_sales = pre_earthquake_sales['sales'].sum()\n",
    "post_earthquake_total_sales = post_earthquake_sales['sales'].sum()\n",
    "\n",
    "# Visualize the sales data before and after the sales data\n",
    "labels = ['Pre-Earthquake', 'Post-Earthquake']\n",
    "total_sales = [pre_earthquake_total_sales, post_earthquake_total_sales]\n",
    "plt.bar(labels, total_sales)\n",
    "plt.xlabel('Before And After Earthquake')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.title('Impact Of Earthquake On Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2a11f",
   "metadata": {},
   "source": [
    "The plot above shows that there was a slight increase in sales after the earthquake. This means that the earthquake did not affect the sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d381900",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Are certain groups of stores selling more products? (Cluster, city, state, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7220fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales in the different store clusters\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='cluster', y='sales', data=train_merged)\n",
    "plt.title('Sales In Different Store Clusters')\n",
    "plt.xlabel('Store Clusters')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21819bb8",
   "metadata": {},
   "source": [
    "The plot above shows that stores in cluster 5 are making more sales than the stores in other clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b860ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales in different cities\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='city', y='sales', data=train_merged)\n",
    "plt.title('Sales In Different Cities')\n",
    "plt.xlabel('Cities')\n",
    "plt.ylabel('Sales')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29bb3b9",
   "metadata": {},
   "source": [
    "The plot above shows that stores in Quito are making more sales than stores in other cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aababf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales in different states\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='state', y='sales', data=train_merged)\n",
    "plt.title('Sales In Different States')\n",
    "plt.xlabel('States')\n",
    "plt.ylabel('Sales')\n",
    "plt.xticks(rotation = 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7077f5f3",
   "metadata": {},
   "source": [
    "The plot above shows that stores in Pichincha are making more sales than stores in other states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales in different store types\n",
    "\n",
    "store_types = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='store_type', y='sales', data=train_merged, order=store_types)\n",
    "plt.title('Sales In Different Store Types')\n",
    "plt.xlabel('Store Type')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a80024",
   "metadata": {},
   "source": [
    "The plot above shows that stores in Store type A are making more sales than stores in other store types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebbf59",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Are sales affected by promotions, oil prices and holidays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify families with at least one item on promotion and families with no item on promotion\n",
    "train_merged['onpromotion'] = train_merged['onpromotion'].apply(lambda x: 'No Promotion' if x == 0 else 'Promotion')\n",
    "\n",
    "# Group by promotion and sum the sales\n",
    "x = train_merged.groupby(['onpromotion'], as_index=False).agg({'sales':'sum'})\n",
    "\n",
    "# Plot the sales of promotion and non_promotion families)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(x.onpromotion, x.sales)\n",
    "plt.title('Impact Of Promotion On Sales')\n",
    "plt.xticks((0,1))\n",
    "plt.xlabel('Promotion Status')\n",
    "plt.ylabel('Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624b7c3",
   "metadata": {},
   "source": [
    "The plot above shows that sales are affected by promotion. Product families with items on promotion are being sold more than product families with no item on promotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bed2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales with different oil prices\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.scatterplot(x='oil_price', y='sales', data=train_merged)\n",
    "plt.title('Sales With Different Oil Prices')\n",
    "plt.xlabel('Oil Prices')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0b8c34",
   "metadata": {},
   "source": [
    "The plot above shows that sales are affected by the oil prices. As shown, there are more number of sales and more volume of sales at lower oil prices than at higher oil prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a9aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the holiday events\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(x='holiday_type', data=train_merged)\n",
    "plt.title('Count of Holiday Events By Type')\n",
    "plt.xlabel('Holiday Events')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales on holidays with sales on non-holidays\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='holiday_type', y='sales', data=train_merged)\n",
    "plt.title('Sales On Holidays Vs Non-holidays')\n",
    "plt.xlabel('Is Holidays')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59394b4",
   "metadata": {},
   "source": [
    "There are more sales during holidays than when there are no holidays. This is because more people are free to go out for shopping during holidays than when there is no holiday and they have to be at work or school."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943bec01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare the sales on holidays with sales on non-holidays for the different store types\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='store_type', hue='holiday_type', y='sales', data=train_merged, ci=None, order=store_types)\n",
    "plt.title('Sales On Holidays Vs Non-holidays For Each Store Type')\n",
    "plt.xlabel('Is Holidays')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend(title='Holiday Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4779c",
   "metadata": {},
   "source": [
    "There is no signicant impact of holidays on the sales made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5ac0a",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What analysis can we get from the date and its extractable features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68232d74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# From the date and its extractable features, the average sales on different days of the week can be analyzed\n",
    "\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='dayname', y='sales', data=train_merged, order=days)\n",
    "plt.title('Sales On Different Days Of The Week')\n",
    "plt.xlabel('Days Of The Week')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2442b5",
   "metadata": {},
   "source": [
    "From the date and its extractable features, we can observe the sales according to the days of the week. As shown above, there are more sales during the weekends, with peak sales on Sundays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ddbf1",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f05791",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "What is the total sales made each year by the corporation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef8e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and sum of sales for each year\n",
    "\n",
    "sales_per_year = train_merged.groupby(['year'], as_index=False).agg({'sales':'sum'})\n",
    "sales_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee58d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sales made each year\n",
    "\n",
    "plt.bar(sales_per_year.year, sales_per_year.sales)\n",
    "plt.title('Sales Per Year',fontsize=14)\n",
    "plt.ylabel('Sales',fontsize=14)\n",
    "plt.xlabel('Year',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef1a67",
   "metadata": {},
   "source": [
    "From the data provided, Corporation Favorita made it's lowest sales in 2013 and its highest sales in 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54de21d",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52463b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the train_merged dataset on which to perform feature engineering\n",
    "\n",
    "train_data = train_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cac279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index back to a normal column\n",
    "\n",
    "train_data = train_data.reset_index()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f0db9",
   "metadata": {},
   "source": [
    "### Drop Unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all the columns of the dataset\n",
    "\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b3556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random sample of 5 rows to see the contents of the columns\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some columns and display the dataset\n",
    "\n",
    "columns_to_drop = ['id', 'store_nbr', 'locale', 'locale_name', 'description', 'transferred', 'city', 'store_type', 'cluster', 'month', 'dayofmonth', 'dayofweek', 'dayname']\n",
    "\n",
    "train_data = train_data.drop(columns_to_drop, axis=1)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511eddc7",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "\n",
    "The dataset will be splitted to training and validation sets using the time-based split. This will be done based on the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the years in the dataset\n",
    "\n",
    "train_data['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the years for the training set and validation set\n",
    "train_years = [2013, 2014, 2015, 2016]\n",
    "val_year = [2017]\n",
    "\n",
    "# Obtain the training set and validation set\n",
    "train_set = train_data.loc[train_data['year'].isin(train_years)]\n",
    "val_set = train_data.loc[train_data['year'].isin(val_year)]\n",
    "\n",
    "# Print the shape of the training set and validation set\n",
    "train_set.shape, val_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the datatypes of the columns of the training set\n",
    "\n",
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the categorical columns to encode and numerical columns to scale\n",
    "cat_columns_to_encode = ['family', 'onpromotion', 'holiday_type', 'state']\n",
    "num_columns_to_scale = ['transactions', 'oil_price']\n",
    "\n",
    "# Create seperate DataFrames for categorical columns and numerical columns of training set\n",
    "train_set_cat_df = train_set[cat_columns_to_encode]\n",
    "train_set_num_df = train_set[num_columns_to_scale]\n",
    "\n",
    "# Create seperate DataFrames for categorical columns and numerical columns of validation set\n",
    "val_set_cat_df = val_set[cat_columns_to_encode]\n",
    "val_set_num_df = val_set[num_columns_to_scale]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6877ec",
   "metadata": {},
   "source": [
    "### Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549da07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an encoder object using OneHotEncoder.\n",
    "# Set sparse=False for dense output and drop='first' to avoid multicollinearity\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to encode the categorical columns of the training set\n",
    "encoder.fit(train_set_cat_df)\n",
    "train_set_cat_encoded = encoder.transform(train_set_cat_df).tolist()\n",
    "train_set_cat_encoded_df = pd.DataFrame(train_set_cat_encoded, columns=encoder.get_feature_names_out())\n",
    "\n",
    "# View the encoded columns of the training set\n",
    "train_set_cat_encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aed355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to encode the categorical columns of the training set\n",
    "encoder.fit(val_set_cat_df)\n",
    "val_set_cat_encoded = encoder.transform(val_set_cat_df).tolist()\n",
    "val_set_cat_encoded_df = pd.DataFrame(val_set_cat_encoded, columns=encoder.get_feature_names_out())\n",
    "\n",
    "# View the encoded columns of the validation set\n",
    "val_set_cat_encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb672032",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89815d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaler object using StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f480e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler to scale the numerical columns of the training set\n",
    "scaler.fit(train_set_num_df)\n",
    "train_set_num_scaled = scaler.transform(train_set_num_df).tolist()\n",
    "train_set_num_scaled_df = pd.DataFrame(train_set_num_scaled, columns=scaler.get_feature_names_out())\n",
    "\n",
    "# View the scaled columns of the training set\n",
    "train_set_num_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed04e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler to scale the numerical columns of the validation set\n",
    "scaler.fit(val_set_num_df)\n",
    "val_set_num_scaled = scaler.transform(val_set_num_df).tolist()\n",
    "val_set_num_scaled_df = pd.DataFrame(val_set_num_scaled, columns=scaler.get_feature_names_out())\n",
    "\n",
    "# View the scaled columns of the validation set\n",
    "val_set_num_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the encoded and scaled DataFrames with the date and sales columns of train_set and val_set\n",
    "# to get the final training and validation sets\n",
    "\n",
    "train_final = pd.concat([train_set_cat_encoded_df, train_set_num_scaled_df, train_set[['date', 'sales']]], axis=1)\n",
    "val_final = pd.concat([val_set_cat_encoded_df, val_set_num_scaled_df, val_set[['date', 'sales']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e121b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc094e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2ad7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191d3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the encoded X_train DataFrame and the scaled X_train DataFrame to have the unbalanced X_train DataFrame\n",
    "\n",
    "X_train = pd.concat([X_train_cat_encoded_df, X_train_num_scaled_df.set_axis(X_train_cat_encoded_df.index)], axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the encoded X_val DataFrame and the scaled X_val DataFrame to have the ready X_val DataFrame\n",
    "\n",
    "X_val = pd.concat([X_val_cat_encoded_df, X_val_num_scaled_df.set_axis(X_val_cat_encoded_df.index)], axis=1)\n",
    "X_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd266dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# series = train_merged.loc[:, 'transactions'].values\n",
    "# series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255da1a",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "The following models will be built and evaluated:\n",
    "1. Linear Regression\n",
    "\n",
    "2. XGBoost\n",
    "\n",
    "3. CatBoost\n",
    "\n",
    "4. ARIMA\n",
    "\n",
    "5. SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf203277",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def evaluate_time_series_models(data, date_column, target_column):\n",
    "#     # Assuming you have your time series data in a DataFrame 'data'\n",
    "#     # Replace 'date_column' with the column containing date/time and 'target_column' with the target variable column name\n",
    "#     data[date_column] = pd.to_datetime(data[date_column])\n",
    "#     data.set_index(date_column, inplace=True)\n",
    "#     target = data[target_column]\n",
    "\n",
    "#     # Splitting the data into training and testing sets (you can adjust the split ratio as per your requirement)\n",
    "#     train_size = int(len(target) * 0.8)\n",
    "#     train, test = target[:train_size], target[train_size:]\n",
    "\n",
    "# Initialize model names and corresponding models\n",
    "model_names = ['Linear Regression', 'XGBoost', 'CatBoost', 'ARIMA', 'SARIMA']\n",
    "models = [LinearRegression(), xgb.XGBRegressor(), CatBoostRegressor(), ARIMA(train, order=(1, 0, 0)),\n",
    "          SARIMAX(train, order=(1, 0, 0), seasonal_order=(1, 0, 0, 12))]\n",
    "\n",
    "# Create a DataFrame to store the evaluation results\n",
    "results = pd.DataFrame(columns=['Model', 'MSE', 'RMSE', 'RMSLE', 'MSLE'])\n",
    "\n",
    "for model_name, model in zip(model_names, models):\n",
    "    if model_name in ['ARIMA', 'SARIMA']:\n",
    "        # Fit the ARIMA or SARIMA model\n",
    "        model_fit = model.fit()\n",
    "\n",
    "        # Make predictions\n",
    "        model_preds = model_fit.forecast(steps=len(test))\n",
    "    else:\n",
    "        # Fit other models\n",
    "        model.fit(np.arange(len(train)).reshape(-1, 1), train)\n",
    "\n",
    "        # Make predictions\n",
    "        model_preds = model.predict(np.arange(len(test)).reshape(-1, 1))\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse = mean_squared_error(test, model_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    rmsle = np.sqrt(mean_squared_log_error(test, model_preds))\n",
    "    msle = mean_squared_log_error(test, model_preds)\n",
    "\n",
    "    # Append the results to the DataFrame\n",
    "    results = results.append({'Model': model_name, 'MSE': mse, 'RMSE': rmse, 'RMSLE': rmsle, 'MSLE': msle},\n",
    "                             ignore_index=True)\n",
    "\n",
    "return results\n",
    "\n",
    "# Assuming you have your time series data in a DataFrame 'data' with columns 'Date' and 'Target'\n",
    "# Adjust the column names accordingly\n",
    "results = evaluate_time_series_models(train_merged, date_column='Date', target_column='y_val')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720fc8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "# Fit the model\n",
    "catboost_model.fit(np.arange(len(X_train)).reshape(-1, 1), X_train)\n",
    "\n",
    "# Make predictions\n",
    "catboost_preds = catboost_model.predict(np.arange(len(X_val)).reshape(-1, 1))\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse_catboost = mean_squared_error(X_val, catboost_preds)\n",
    "rmse_catboost = np.sqrt(mse_catboost)\n",
    "rmsle_catboost = np.sqrt(mean_squared_log_error(X_val, catboost_preds))\n",
    "msle_catboost = mean_squared_log_error(X_val, catboost_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d33545",
   "metadata": {},
   "source": [
    "### AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pacf and acf\n",
    "\n",
    "pacf = plot_pacf(data['sales'], lags=10)\n",
    "acf = plot_acf(data['sales'], lags=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoReg(data, lags=5).fit()\n",
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb18a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AR_pred = model.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c43257",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(train['sales'], label='Train')\n",
    "plt.plot(val['sales'], label='Validation')\n",
    "plt.plot(AR_pred, label='AR Forecast')\n",
    "plt.title('AutoRegressive (AR) Method')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "results = pd.DataFrame(columns=['Model', 'MSE', 'RMSE', 'RMSLE', 'MSLE'])\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(val, AR_pred)\n",
    "msle = mean_squared_log_error(val, AR_pred)\n",
    "rmse = np.sqrt(mse).round(2)\n",
    "rmsle = np.sqrt(msle).round(2)\n",
    "\n",
    "# Create a new row for the evaluation metrics of ARIMA model on the results DataFrame\n",
    "ARIMA_results = pd.DataFrame([['AR', 'mse', 'rmse', 'rmsle', 'msle']], columns=['Model', 'MSE', 'RMSE', 'RMSLE', 'MSLE'])\n",
    "results = results.append(ARIMA_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07e55b",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d025217",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_fit = autoarima(train['sales'], trace=True, suppress_warnings=True)\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50fda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(train, order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809315a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the validation set\n",
    "\n",
    "ARIMA_pred = model.predict(start=len(train), end=len(train)+len(test)-1, typ='levels')\n",
    "ARIMA_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(train['sales'], label='Train')\n",
    "plt.plot(val['sales'], label='Validation')\n",
    "plt.plot(ARIMA_pred, label='ARIMA Forecast')\n",
    "plt.title('AutoRegressive Integrated Moving Average (ARIMA) method')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(val, ARIMA_pred)\n",
    "msle = mean_squared_log_error(val, ARIMA_pred)\n",
    "rmse = np.sqrt(mse).round(2)\n",
    "rmsle = np.sqrt(msle).round(2) \n",
    "\n",
    "# Create a new row for the evaluation metrics of ARIMA model on the results DataFrame\n",
    "ARIMA_results = pd.DataFrame([['ARIMA', 'mse', 'rmse', 'rmsle', 'msle']], columns=['Model', 'MSE', 'RMSE', 'RMSLE', 'MSLE'])\n",
    "results = results.append(ARIMA_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48fcc57",
   "metadata": {},
   "source": [
    "### SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40272c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "model = SARIMAX(train, order=(1,1,1), seasonal_order=(1,1,1,12))\n",
    "model_fit = model.fit()\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f8197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the validation set\n",
    "\n",
    "SARIMA_pred = model.predict(start=len(train), end=len(train)+len(test)-1, typ='levels')\n",
    "SARIMA_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(train['sales'], label='Train')\n",
    "plt.plot(val['sales'], label='Validation')\n",
    "plt.plot(SARIMA_pred, label='SARIMA Forecast')\n",
    "plt.title('Seasonal AutoRegressive Integrated Moving Average (SARIMA) method')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(val, SARIMA_pred)\n",
    "msle = mean_squared_log_error(val, SARIMA_pred)\n",
    "rmse = np.sqrt(mse).round(2)\n",
    "rmsle = np.sqrt(msle).round(2) \n",
    "\n",
    "# Create a new row for the evaluation metrics of ARIMA model on the results DataFrame\n",
    "SARIMA_results = pd.DataFrame([['SARIMA', 'mse', 'rmse', 'rmsle', 'msle']], columns=['Model', 'MSE', 'RMSE', 'RMSLE', 'MSLE'])\n",
    "results = results.append(SARIMA_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5321872",
   "metadata": {},
   "source": [
    "# Make series stationary\n",
    "\n",
    "We will use the boxcox method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32826081",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_boxcox = pd.Series(boxcox(train['sales'], lbda=0), index=train.index)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(data_boxcox, label='Before Boxcox Transformation')\n",
    "plt.title('Before Boxcox Transformation')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c35e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_boxcox_diff = pd.Series(data_boxcox - data_boxcox.shift(), index=train.index)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(data_boxcox_diff, label='After Boxcox Transformation')\n",
    "plt.title('After Boxcox Transformation')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af92ed9",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad0bde",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cdc858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
